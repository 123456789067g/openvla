from fastapi import FastAPI, UploadFile, File, Form
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import torch
import uvicorn
import io
 
# ========== å¯é€‰ï¼šæ”¯æŒ HEIC ==========
try:
    import pillow_heif
    pillow_heif.register_heif_opener()
except ImportError:
    print("âš ï¸ æ²¡æœ‰å®‰è£… pillow-heifï¼ŒHEIC å¯èƒ½æ‰“ä¸å¼€ã€‚è¿è¡Œ: pip install pillow-heif")
 
# ========== è®¾å¤‡é€‰æ‹© ==========
device = "cuda:0" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device.startswith("cuda") else torch.float32
print(f"ğŸ‘‰ ä½¿ç”¨è®¾å¤‡: {device}, æ•°æ®ç±»å‹: {dtype}")
 
# ========== åŠ è½½æ¨¡å‹ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰ ==========
processor = AutoProcessor.from_pretrained("openvla/openvla-7b", trust_remote_code=True)
vla = AutoModelForVision2Seq.from_pretrained(
    "openvla/openvla-7b",
    torch_dtype=dtype,
    low_cpu_mem_usage=True,
    trust_remote_code=True
).to(device)
 
# ========== FastAPI åº”ç”¨ ==========
app = FastAPI()
 
@app.post("/infer")
async def infer(
    file: UploadFile = File(...),
    instruction: str = Form(...)
):
    """
    ä¸Šä¼ å›¾ç‰‡ + æŒ‡ä»¤ï¼Œè¿”å› OpenVLA åŠ¨ä½œç»“æœ
    """
    # 1. è¯»å–ä¸Šä¼ çš„å›¾ç‰‡
    contents = await file.read()
    image = Image.open(io.BytesIO(contents)).convert("RGB")
 
    # 2. æ„é€  prompt
    prompt = f"In: {instruction}\nOut:"
 
    # 3. æ¨¡å‹å¤„ç†
    inputs = processor(prompt, image).to(device, dtype=dtype)
    action = vla.predict_action(**inputs, unnorm_key="bridge_orig", do_sample=False)
 
    # 4. è¿”å› JSON
    return {"action": action.tolist()}  # è½¬ list ä¾¿äº JSON åºåˆ—åŒ–
 
# ========== å¯åŠ¨å…¥å£ ==========
if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)