from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import torch

# ========== å¯é€‰ï¼šæ”¯æŒ HEIC ==========
try:
    import pillow_heif
    pillow_heif.register_heif_opener()
except ImportError:
    print("âš ï¸ æ²¡æœ‰å®‰è£… pillow-heifï¼ŒHEIC å¯èƒ½æ‰“ä¸å¼€ã€‚è¿è¡Œ: pip install pillow-heif")

# ========== è®¾å¤‡é€‰æ‹© ==========
device = "cuda:0" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device.startswith("cuda") else torch.float32
print(f"ğŸ‘‰ ä½¿ç”¨è®¾å¤‡: {device}, æ•°æ®ç±»å‹: {dtype}")

# ========== åŠ è½½æ¨¡å‹ ==========
processor = AutoProcessor.from_pretrained("openvla/openvla-7b", trust_remote_code=True)
vla = AutoModelForVision2Seq.from_pretrained(
    "openvla/openvla-7b",
    torch_dtype=dtype,
    low_cpu_mem_usage=True,
    trust_remote_code=True
).to(device)

# ========== åŠ è½½ç…§ç‰‡ ==========
image_path = "picture/IMG_2581.HEIC"   # ä¿®æ”¹æˆä½ è‡ªå·±çš„è·¯å¾„
image = Image.open(image_path)

# ========== Prompt ==========
prompt = "In: What action should the robot take to pick up the object?\nOut:"

# ========== å¤„ç†è¾“å…¥ ==========
inputs = processor(prompt, image).to(device, dtype=dtype)

# ========== æ¨ç† ==========
action = vla.predict_action(**inputs, unnorm_key="bridge_orig", do_sample=False)
print("Predicted action:", action)
