from fastapi import FastAPI, UploadFile, File
from PIL import Image
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
import io
 
# ========== åˆå§‹åŒ–æ¨¡å‹ ==========
device = "cuda:0" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device.startswith("cuda") else torch.float32
print(f"ğŸ‘‰ ä½¿ç”¨è®¾å¤‡: {device}, æ•°æ®ç±»å‹: {dtype}")
 
processor = AutoProcessor.from_pretrained("openvla/openvla-7b", trust_remote_code=True)
vla = AutoModelForVision2Seq.from_pretrained(
    "openvla/openvla-7b",
    torch_dtype=dtype,
    low_cpu_mem_usage=True,
    trust_remote_code=True
).to(device)
 
# ========== å»º API åº”ç”¨ ==========
app = FastAPI()
 
@app.post("/infer")
async def infer(file: UploadFile = File(...)):
    # 1. è¯»å–ä¸Šä¼ çš„å›¾åƒ
    contents = await file.read()
    image = Image.open(io.BytesIO(contents))
 
    # 2. Promptï¼ˆä½ å¯ä»¥æ¢æˆè‡ªå·±çš„ï¼‰
    prompt = "In: Catch the black object\nOut:"
 
    # 3. å¤„ç†è¾“å…¥
    inputs = processor(prompt, image).to(device, dtype=dtype)
 
    # 4. æ¨¡å‹æ¨ç†
    action = vla.predict_action(**inputs, unnorm_key="bridge_orig", do_sample=False)
 
    # 5. è¿”å›ç»“æœ
    return {"predicted_action": action}